\documentclass[8pt]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\title{Machine Learning Outline}
\author{Jordan E Dehmel}
\date{}

\begin{document}
\maketitle

\section{Gradient Descent}

We seek to change the weights and biases of each node to reduce the
global error of the network. An easy way to do this is **gradient descent**.
We must first calculate the gradient vector of the error with respect to
each weight. By the definition of a gradient, each entry of this vector will
be equal to the partial derivative of the error with respect to some weight,
which will be calculated via the chain rule for partial derivatives.

Pseudocode for finding the derivative of some node with respect to
some weight $w_k$:
\begin{verbatim}
if w_k is in links:
    return links[w_k].previousActivation
else:
    out = 0
    for link in links:
        selfDer = (derivative of this with respect to link)
        out += selfDer * (derivative of link with respect to w_k)
    return out
\end{verbatim}

Anatomy of an error node:

Method 1 (vector norm):
$$
\begin{aligned}
    y &= \sqrt{\sum{(e_i - o_i)^2}} \\
    \frac{\partial y}{\partial w_k} &= \frac{-\sum{(e_i - o_i)\frac{\partial o_i}{\partial w_k}}}{y}
\end{aligned}
$$

Method 2 (easy derivative):
$$
\begin{aligned}
    y &= \sum{(e_i - o_i)^2} \\
    \frac{\partial y}{\partial w_k} &= -2 \sum{(e_i - o_i) \frac{\partial o_i}{\partial w_k}}
\end{aligned}
$$

Anatomy of a regular node:

Sigmoid $S$:
$$
\begin{aligned}
    S(x) &= \frac{1}{1 + e^{-x}} \\
    S'(x) &= S(x) (1 - S(x))
\end{aligned}
$$

Sum $f$:
$$
\begin{aligned}
    f(\vec{b}) &= \sum{v_i w_i} + b \\
    \frac{\partial f(\vec{b})}{\partial w_k} &= \sum{w_i \frac{\partial v_i}{\partial w_k}}
\end{aligned}
$$
(If $w_k$ is within its list of weights, the derivative will instead be equal to $v_i$)

Full equation for a regular node:
$$
\begin{aligned}
    y &= S(f(\vec{b})) \\
    \frac{\partial y}{\partial s} &= S'(f(\vec{b}))(f'(\vec{b}))
\end{aligned}
$$
With inputs $v_n$, weights $w_i$ and bias $b$.
Partial derivative taken with respect to later weight $s$.

The error dummy node represents a function of $N$ variables, where $N$ is the
number of weights in the network. To perform gradient descent on the error, we
must of course find the gradient. This is a vector of $N$ dimensions where the
$i$th entry is the derivative of the error with respect to the $i$th weight.
Once the gradient is found, we will move some amount backwards in its direction.
This amounts to decrementing each weight by its corrosponding gradient entry
times some scalar.

\section{Stochastic Gradient Descent}

This above method has a low rate of convergence. This can be fixed by
introducing an aspect of randomness to our gradient descent. Instead of only
using the unit gradient vector, we can use this times some random icrement. This
allows it to jump out of local error minima in the pursuit of global minima.

\section{The Network Pool and Periodic Pruning}

Even using stochastic gradient descent, networks will only sometimes converge.
This happens when the gradient finds itself stuck in a local error minima that is
too deep to escape from, even through stochastic means. Thus, we introduce
network pools and pruning, two heuristic methods of network improvement.

A **network pool** holds several clones of a network. It trains all of these on
seperate threads, and checks in periodically. At each check, it **prunes** some
number of the worst-performing networks (deletes them). It then repopulates the
pool with clones of the best-performing network. During the cloning process, all
the weights in the clone are modified by some random amount to shove their
error gradients out of local minima. This is also beneficial due to its multi-
threading, which allows capable machines to gain better improvements in network
accuracy over shorter periods of time. A good unit for the measurement of this
is thus the error-nanosecond, computed as the product of the final network error
and the ellapsed nanoseconds of training, where a lower number is better.

These methods constitute a heuristic because the random modulation of weights
may not neccessarily improve the network's error. If an ideal network is found
as the root of the pool, there is no point in the cloning and modulating process.
However, this is much less likely to occur than local error minima.

\section{Error-NS}

We will apply our Error-NS measurement to the regular and network pool training
methods for a 2/10/2 network training to emulate a XOR and AND gate. The network
pool had a pool size of 10 with a pruning rate of \%70 every 10 passes. Both
networks began randomized and trained for 1000 passes.

\subsection{Trial 1}

The regular training method failed to converge.
Regular Error-NS: 3.51 e9
Pool Error-NS: 1.42 e-26
Ratio of pool to regular: 2.47 e35 (35 orders of magnitude)

\subsection{Trial 2}

The regular training method yielded a high error rate.
Regular Error-NS: 3.05 e9
Pool Error-NS: 2.74 e-49
Ratio of pool to regular: 1.11 e58 (58 orders of magnitude)

\subsection{Trial 3}

The regular training method yielded a high error rate.
Regular Error-NS: 2.00 e9
Pool Error-NS: 3.78 e-51
Ratio of pool to regular: 5.29 e59 (59 orders of magnitude)

\subsection{Trial 4}

The regular training method failed to converge.
Regular Error-NS: 3.78 e9
Pool Error-NS: 1.14 e-33
Ratio of pool to regular: 3.31 e42 (42 orders of magnitude)

\subsection{Trial 5}

The regular training method failed to converge.
Regular Error-NS: 3.69 e9
Pool Error-NS: 2.33 e-69
Ratio of pool to regular: 1.58 e78 (78 orders of magniture)

\section{Analysis}

Although it seems unfair to compare this dataset, upon which the regular
training process converged properly 0 times, this is representative and
sequentially recorded data. Note that these tests were run on a low-end
converted chromebook with only 4 threads. On a better machine, the
network pool would perform even better due to its abaility to be multithreaded.

\end{document}

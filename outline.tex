\documentclass[8pt]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\title{Machine Learning Outline}
\author{Jordan E Dehmel}
\date{}

\begin{document}
\maketitle

\section{Gradient Descent}

We seek to change the weights and biases of each node to reduce the
global error of the network. An easy way to do this is **gradient descent**.
We must first calculate the gradient vector of the error with respect to
each weight. By the definition of a gradient, each entry of this vector will
be equal to the partial derivative of the error with respect to some weight,
which will be calculated via the chain rule for partial derivatives.

Pseudocode for finding the derivative of some node with respect to
some weight $w_k$:
\begin{verbatim}
if w_k is in links:
    return links[w_k].previousActivation
else:
    out = 0
    for link in links:
        selfDer = (derivative of this with respect to link)
        out += selfDer * (derivative of link with respect to w_k)
    return out
\end{verbatim}

Anatomy of an error node:

Method 1 (vector norm):
$$
\begin{aligned}
    y &= \sqrt{\sum{(e_i - o_i)^2}} \\
    \frac{\partial y}{\partial w_k} &= \frac{-\sum{(e_i - o_i)\frac{\partial o_i}{\partial w_k}}}{y}
\end{aligned}
$$

Method 2 (easy derivative):
$$
\begin{aligned}
    y &= \sum{(e_i - o_i)^2} \\
    \frac{\partial y}{\partial w_k} &= -2 \sum{(e_i - o_i) \frac{\partial o_i}{\partial w_k}}
\end{aligned}
$$

Anatomy of a regular node:

Sigmoid $S$:
$$
\begin{aligned}
    S(x) &= \frac{1}{1 + e^{-x}} \\
    S'(x) &= S(x) (1 - S(x))
\end{aligned}
$$

Sum $f$:
$$
\begin{aligned}
    f(\vec{b}) &= \sum{v_i w_i} + b \\
    \frac{\partial f(\vec{b})}{\partial w_k} &= \sum{w_i \frac{\partial v_i}{\partial w_k}}
\end{aligned}
$$
(If $w_k$ is within its list of weights, the derivative will instead be equal to $v_i$)

Full equation for a regular node:
$$
\begin{aligned}
    y &= S(f(\vec{b})) \\
    \frac{\partial y}{\partial s} &= S'(f(\vec{b}))(f'(\vec{b}))
\end{aligned}
$$
With inputs $v_n$, weights $w_i$ and bias $b$.
Partial derivative taken with respect to later weight $s$.

The error dummy node represents a function of $N$ variables, where $N$ is the
number of weights in the network. To perform gradient descent on the error, we
must of course find the gradient. This is a vector of $N$ dimensions where the
$i$th entry is the derivative of the error with respect to the $i$th weight.
Once the gradient is found, we will move some amount backwards in its direction.
This amounts to decrementing each weight by its corrosponding gradient entry
times some scalar.

\section{Stochastic Gradient Descent}

This above method has a low rate of convergence. This can be fixed by
introducing an aspect of randomness to our gradient descent. Instead of only
using the unit gradient vector, we can use this times some random icrement. This
allows it to jump out of local error minima in the pursuit of global minima.

\section{The Network Pool and Periodic Pruning}

Even using stochastic gradient descent, networks will only sometimes converge.
This happens when the gradient finds itself stuck in a local error minima that is
too deep to escape from, even through stochastic means. Thus, we introduce
network pools and pruning.

A **network pool** holds several clones of a network. It trains all of these on
seperate threads, and checks in periodically. At each check, it **prunes** some
number of the worst-performing networks (deletes them). It then repopulates the
pool with clones of the best-performing network. During the cloning process, all
the weights in the clone are modified by some random amount to shove their
error gradients out of local minima.

\end{document}

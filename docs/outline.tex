\documentclass[8pt]{amsart}

\usepackage{geometry}
\geometry{letterpaper}

\title{Machine Learning Outline}
\author{Jordan E Dehmel}
\date{}

\begin{document}
\maketitle

\section{Gradient Descent}

We seek to change the weights and biases of each node to reduce the
global error of the network. An easy way to do this is **gradient descent**.
We must first calculate the gradient vector of the error with respect to
each weight. By the definition of a gradient, each entry of this vector will
be equal to the partial derivative of the error with respect to some weight,
which will be calculated via the chain rule for partial derivatives.

Pseudocode for finding the derivative of some node with respect to
some weight $w_k$:
\begin{verbatim}
if w_k is in links:
    return links[w_k].previousActivation
else:
    out = 0
    for link in links:
        selfDer = (derivative of this with respect to link)
        out += selfDer * (derivative of link with respect to w_k)
    return out
\end{verbatim}

Anatomy of an error node:

Method 1 (vector norm):
$$
\begin{aligned}
    y &= \sqrt{\sum{(e_i - o_i)^2}} \\
    \frac{\partial y}{\partial w_k} &= \frac{-\sum{(e_i - o_i)\frac{\partial o_i}{\partial w_k}}}{y}
\end{aligned}
$$

Method 2 (easy derivative):
$$
\begin{aligned}
    y &= \sum{(e_i - o_i)^2} \\
    \frac{\partial y}{\partial w_k} &= -2 \sum{(e_i - o_i) \frac{\partial o_i}{\partial w_k}}
\end{aligned}
$$

Anatomy of a regular node:

Sigmoid $S$:
$$
\begin{aligned}
    S(x) &= \frac{1}{1 + e^{-x}} \\
    S'(x) &= S(x) (1 - S(x))
\end{aligned}
$$

Sum $f$:
$$
\begin{aligned}
    f(\vec{b}) &= \sum{v_i w_i} + b \\
    \frac{\partial f(\vec{b})}{\partial w_k} &= \sum{w_i \frac{\partial v_i}{\partial w_k}}
\end{aligned}
$$
(If $w_k$ is within its list of weights, the derivative will instead be equal to $v_i$)

Full equation for a regular node:
$$
\begin{aligned}
    y &= S(f(\vec{b})) \\
    \frac{\partial y}{\partial s} &= S'(f(\vec{b}))(f'(\vec{b}))
\end{aligned}
$$
With inputs $v_n$, weights $w_i$ and bias $b$.
Partial derivative taken with respect to later weight $s$.

The error dummy node represents a function of $N$ variables, where $N$ is the
number of weights in the network. To perform gradient descent on the error, we
must of course find the gradient. This is a vector of $N$ dimensions where the
$i$th entry is the derivative of the error with respect to the $i$th weight.
Once the gradient is found, we will move some amount backwards in its direction.
This amounts to decrementing each weight by its corrosponding gradient entry
times some scalar.

\section{Stochastic Gradient Descent}

This above method has a low rate of convergence. This can be fixed by
introducing an aspect of randomness to our gradient descent. Instead of only
using the unit gradient vector, we can use this times some random icrement. This
allows it to jump out of local error minima in the pursuit of global minima.

\section{The Network Pool and Periodic Pruning}

Even using stochastic gradient descent, networks will only sometimes converge.
This happens when the gradient finds itself stuck in a local error minima that is
too deep to escape from, even through stochastic means. Thus, we introduce
network pools and pruning, two heuristic methods of network improvement.

A **network pool** holds several clones of a network. It trains all of these on
seperate threads, and checks in periodically. At each check, it **prunes** some
number of the worst-performing networks (deletes them). It then repopulates the
pool with clones of the best-performing network. During the cloning process, all
the weights in the clone are modified by some random amount to shove their
error gradients out of local minima. This is also beneficial due to its multi-
threading, which allows capable machines to gain better improvements in network
accuracy over shorter periods of time. A good unit for the measurement of this
is thus the error-nanosecond, computed as the product of the final network error
and the ellapsed nanoseconds of training, where a lower number is better.

These methods constitute a heuristic because the random modulation of weights
may not neccessarily improve the network's error. If an ideal network is found
as the root of the pool, there is no point in the cloning and modulating process.
However, this is much less likely to occur than local error minima.

\section{Error-NS}

We will apply our Error-NS measurement to the regular and network pool training
methods for a 2/10/2 network training to emulate a XOR and AND gate. The network
pool had a pool size of 10 with a pruning rate of \%70 every 10 passes. Both
networks began randomized and trained for 1000 passes.

\subsection{Trial 1}

The regular training method failed to converge.
Regular Error-NS: 3.51 e9
Pool Error-NS: 1.42 e-26
Ratio of pool to regular: 2.47 e35 (35 orders of magnitude)

\subsection{Trial 2}

The regular training method yielded a high error rate.
Regular Error-NS: 3.05 e9
Pool Error-NS: 2.74 e-49
Ratio of pool to regular: 1.11 e58 (58 orders of magnitude)

\subsection{Trial 3}

The regular training method yielded a high error rate.
Regular Error-NS: 2.00 e9
Pool Error-NS: 3.78 e-51
Ratio of pool to regular: 5.29 e59 (59 orders of magnitude)

\subsection{Trial 4}

The regular training method failed to converge.
Regular Error-NS: 3.78 e9
Pool Error-NS: 1.14 e-33
Ratio of pool to regular: 3.31 e42 (42 orders of magnitude)

\subsection{Trial 5}

The regular training method failed to converge.
Regular Error-NS: 3.69 e9
Pool Error-NS: 2.33 e-69
Ratio of pool to regular: 1.58 e78 (78 orders of magniture)

\section{Analysis}

Although it seems unfair to compare this dataset, upon which the regular
training process converged properly 0 times, this is representative and
sequentially recorded data. Note that these tests were run on a low-end
converted chromebook with only 4 threads. On a better machine, the
network pool would perform even better due to its abaility to be multithreaded.

\section{Efficiency}

This model works. However, it is extremely inneficient. For a network
of dimensions $[64, 20, 1]$ (used for the extremely trivial case of a
8x8-pixel image with only light hidden layering), a single training pass
takes almost 2 seconds. This is unacceptable. This motivates us to seeks more
efficient algorithms.

\section{Linear Algebra}

One may note that the weighted sum of a node ($\sum{w_j I_j} + b$ for items $0-j$
of the weights $w$ and inputs $I$ with bias $b$) is equal to the dot product
of the collumn vectors representing the inputs and the weights. Each entry in
the input vector is in turn the sigmoid-normalized value of another dot product.

To utilize this fact we must step away from our object-oriented approach and
instead represent the network as a set of vectors. We may represent the activations
of one layer of the network as a collumn vector, and the set of all activations as
an array of such vectors. Since each node in the network has its own associated
weights and bias, these will be stored in an auxilery data struction.

While the activation data structure was a vector of vectors, the weights will be
a vector of matrices, with each matrix representing the weights of a layer.

The set of activations:

\[
\left\{
    \begin{bmatrix}
        a_{0, 0} \\
        a_{0, 1} \\
        \cdots{} \\
        a_{0, L_0} \\
    \end{bmatrix},
    \cdots{},
    \begin{bmatrix}
        a_{i, 0} \\
        a_{i, 1} \\
        \cdots{} \\
        a_{i, L_i} \\
    \end{bmatrix} \vert
    \left[ L_0, L_1, \cdots{}, L_i \right]
\right\}
\]

Or, succinctly,

\[
\left\{
    \vec{a_0},
    \cdots,
    \vec{a_i}
\right\}
\]

The set of weights:

\[
\left\{
    W_0, W_1, \cdots, W_L
\right\}
\]

Where each $W_j$ are the weights of the $j$th layer, as below.

\[
    W_j =
    \begin{bmatrix}
        w_{0, 0} & \cdots & w_{0, l} & w_{0, l + 1} \\
        w_{1, 0} & \cdots & w_{1, l} & w_{1, l + 1} \\
        \vdots   & \ddots & \vdots   & \vdots       \\
        w_{k, 0} & \cdots & w_{k, l} & w_{k, l + 1} \\
    \end{bmatrix}
\]

Where $j$ is the current layer, $l$ is the height of the layer below (the relative
input layer), and $k$ is the height of the current layer $j$.

Each row in this matrix represents a source node in the current layer, and each
collumn (except the final one) represents a destination node in the layer below.
The intersection of a collumn and a row is the weight that connects these two nodes.
The final collumn in the matrix represents the bias of the node, which can be seen
as a weight connected to a "ghost node" whose activation is always 1.

We can visualize the weights of an entire network as a 3-dimensional array. The first
major axis of the array represents the layer within the  (starting at 1). Each item
along this dimension is a 2-dimensional array.

Of this 2-dimensional array, the major axis represents the source node. Each item
along this dimension is a 1-dimensional array of numbers.

Finally, in this array, the $n$th item represents the weight from the source node
to the $n$th node in the layer beneath it (or "ghost node" if it is the final entry).

The following pseudo-Python gives an example.

\begin{verbatim}
# This defines the dimensions of the network.
# The first entry is the size of the input layer,
# and the last is the size of the output layer.
dimensions = [3, 2, 4]

# The weights of the network with the above dimensions
# (Note that the input layer has no weights)
weights = [
    # The weights for the hidden layer
    [
        # The weights for the first node in this layer
        [
            1, # The weight connected to the first input node
            2, # To the seconds input node
            3,
            
            4  # The bias
        ],

        # The weights for the second node
        # (written concisely)
        [5, 6, 7, 8]
    ],

    # The weights for the output layer
    # (written consisely)
    [[9, 1, 2],
    [3, 4, 5],
    [6, 7, 8],
    [9, 1, 2]]
]
\end{verbatim}

By representing the weights as a matrix, we can express the weighted sum portion $s$
of a layer $l$ as the dot product $\vec{s_l} = W_l \dot \vec{a_{l - 1}}$. We then
define the output of the activation function such that
$f: \mathbb{R}^2 \to \mathbb{R}^2$ as the item-wise application of $f$ for
all elements of the input.

Thus, we can express the activation of a layer $l$ as the following.

\[
    \vec{a_l} = f(W_l \dot \vec{a_{l - 1}})
\]

This allows the output of the network $\vec{a_L}$ to be written as the repeated
application of this process as follows.

\[
    \vec{a_L} = f \left( 
        W_L \dot f \left(
            W_{L - 1} \dot f \left( 
                W_{L - 2} \dot \cdots
                    f \left( W_0 \vec{I}
                \right) \cdots 
            \right)
        \right)
    \right)
\]

For some input vector $\vec{I}$ on a network with $L$ layers. This is a more
succinct version of the formula we derived before. We define the error $C$ as
before as the following.

\[
    C = \sum E^2 = \sum (e - o)^2 = (e_0 - o_0)^2 + \cdots + (e_i - o_i)^2
\]

We seek to find an efficient computational way to find the gradient of $C$ with
respect to all the weights and biases of the network.

\section{Analysis}

After implementing our Linear Algebra based version neural network, we find that
it runs extremely fast.

For a traditional network of dimensions $[64, 20, 1]$ (used for the extremely
trivial case of a 8x8-pixel image with only light hidden layering), a single training
pass takes almost 2 seconds (2,000,000,000 nanoseconds per pass).

For a linear algebra network of the same dimensions, ten training passes take only
9,423,364 nanoseconds (942,336.4 nanoseconds per pass). This means that our improved
model runs $\frac{2000000000}{942336.4} \equiv 2122.384$ times faster. This, needless
to say, is an incredible improvement and wastes much less computation.

In fact, a multithreaded pool of 10 of these networks can train for 20 passes in the
same time it took our original method to train one network once. Much more significantly,
our Order of complexity is now much better (through observation). This allows us to
train vastly larger models. In the same time it used to take us to train a $[64, 20, 1]$
network once, we can now train a $[16384, 20, 1]$ model (a 128x128-pixel image) 10 times.

\section{Sources}

https://en.wikipedia.org/wiki/Backpropagation \\
http://neuralnetworksanddeeplearning.com/chap2.html

\end{document}
